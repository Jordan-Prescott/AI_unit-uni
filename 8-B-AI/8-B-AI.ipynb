{"cells":[{"cell_type":"code","execution_count":null,"id":"61e7e165","metadata":{"id":"61e7e165"},"outputs":[],"source":["# mount your Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"14e532b0","metadata":{"id":"14e532b0"},"outputs":[],"source":["# change current working directory\n","import os\n","os.chdir('/content/drive/MyDrive/AI/8-B-AI/')"]},{"cell_type":"code","execution_count":null,"id":"25b4bdca","metadata":{"id":"25b4bdca"},"outputs":[],"source":["# check we can see the dataset\n","os.path.isfile('iris.csv')"]},{"cell_type":"code","execution_count":null,"id":"9bb271d0","metadata":{"id":"9bb271d0"},"outputs":[],"source":["# try some visualisation with PCA\n","\n","# data preparation steps\n","\n","# Importing the packages we use\n","import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Loading all the observations\n","observations = pd.read_csv('iris.csv')\n","\n","# Let's assume an unsupervised learning problem, with no labels known\n","datapoints = observations.drop(columns='species').to_numpy()\n","\n","# Create a PCA model object\n","pca = PCA()\n","\n","# Use it to process our datapoints\n","pca.fit(datapoints)\n","\n","# Plot the cumulative explained variance against the number of PCs\n","plt.figure()\n","plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_)*100)\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Cumulative Explained Variance vs Number of Principal Components')\n","plt.grid(True)\n","plt.show()\n","\n","# Transform the original datapoints to the PCA space\n","datapoints = pca.transform(datapoints)\n","\n","# 3D Scatter plot of the datapoints with respect to the first 3 principal components\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","ax.scatter(datapoints[:, 0], datapoints[:, 1], datapoints[:, 2])\n","ax.set_xlabel('Principal Component 1')\n","ax.set_ylabel('Principal Component 2')\n","ax.set_zlabel('Principal Component 3')\n","ax.set_title('3D Scatter Plot of Datapoints in PCA Space')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"1b4280d1","metadata":{"id":"1b4280d1"},"outputs":[],"source":["# try some supervised learning with PCA\n","\n","# data preparation steps\n","\n","# Importing the packages we use\n","import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import matplotlib.pyplot as plt\n","from sklearn.inspection import DecisionBoundaryDisplay\n","import seaborn as sns\n","\n","# Loading all the observations\n","observations = pd.read_csv('iris.csv')\n","\n","# Shuffling all the observations\n","observations_shuffled = observations.sample(frac=1, random_state=99)\n","\n","# Setting the fraction of observations we will use for testing\n","testing_fraction = 0.25\n","split_index = int(observations_shuffled.shape[0] * testing_fraction)\n","\n","# Splitting into testing observations and training observations (\"horizontal split\")\n","observations_test = observations_shuffled.iloc[:split_index]\n","observations_train = observations_shuffled.iloc[split_index:]\n","\n","# Splitting into testing examples and testing labels (\"vertical split\")\n","test_examples = observations_test.drop(columns='species').to_numpy()\n","test_labels = observations_test['species'].to_numpy()\n","\n","# Splitting into training examples and training labels (\"vertical split\")\n","train_examples = observations_train.drop(columns='species').to_numpy()\n","train_labels = observations_train['species'].to_numpy()\n","\n","# Apply PCA to our training data\n","\n","# Create a PCA model object\n","pca = PCA()\n","\n","# Use it to process our training examples\n","pca.fit(train_examples)\n","\n","# Plot the cumulative explained variance against the number of PCs\n","plt.figure()\n","plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_)*100)\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Cumulative Explained Variance vs Number of Principal Components')\n","plt.grid(True)\n","plt.show()\n","\n","# Let's assume we decide to use the first 2 PCs based on the plot above...\n","\n","# Create a PCA model object that only retains the first 2 PCs\n","pca = PCA(n_components=2)\n","\n","# Use it to process our training examples\n","pca.fit(train_examples)\n","\n","# Transform the original training examples to the new 2D space\n","train_examples = pca.transform(train_examples)\n","\n","# Transform the original testing examples to the new 2D space\n","test_examples = pca.transform(test_examples)\n","\n","# model training and model evaluation steps\n","\n","# Create a Decision Tree model object:\n","model = DecisionTreeClassifier(random_state=99)\n","\n","# Call the model's fitting algorithm, passing in our training examples and training labels\n","model.fit(train_examples, train_labels)\n","\n","# Use the trained model to generate predictions for our testing examples\n","predictions = model.predict(test_examples)\n","\n","# Find the total number of model predictions that matched with the corresponding testing labels\n","correct_predictions = sum(predictions == test_labels)\n","# Calculate the model's accuracy: the fraction of predictions that were correct\n","accuracy = correct_predictions / len(test_labels)\n","# Display the accuracy as a single quantitative measure of overall performance\n","print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n","\n","# visualise the final generalisation\n","plt.figure()\n","disp = DecisionBoundaryDisplay.from_estimator(model, train_examples)\n","sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels)\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title('Decision tree: final generalisation')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"cc7b7da5","metadata":{"id":"cc7b7da5"},"outputs":[],"source":["# try some supervised learning with PCA (manual approach, showing the matrix multiplications)\n","\n","# data preparation steps\n","\n","# Importing the packages we use\n","import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","import matplotlib.pyplot as plt\n","from sklearn.inspection import DecisionBoundaryDisplay\n","import seaborn as sns\n","\n","# Loading all the observations\n","observations = pd.read_csv('iris.csv')\n","\n","# Shuffling all the observations\n","observations_shuffled = observations.sample(frac=1, random_state=99)\n","\n","# Setting the fraction of observations we will use for testing\n","testing_fraction = 0.25\n","split_index = int(observations_shuffled.shape[0] * testing_fraction)\n","\n","# Splitting into testing observations and training observations (\"horizontal split\")\n","observations_test = observations_shuffled.iloc[:split_index]\n","observations_train = observations_shuffled.iloc[split_index:]\n","\n","# Splitting into testing examples and testing labels (\"vertical split\")\n","test_examples = observations_test.drop(columns='species').to_numpy()\n","test_labels = observations_test['species'].to_numpy()\n","\n","# Splitting into training examples and training labels (\"vertical split\")\n","train_examples = observations_train.drop(columns='species').to_numpy()\n","train_labels = observations_train['species'].to_numpy()\n","\n","# Apply PCA to our training data\n","\n","# Create a PCA model object\n","pca = PCA()\n","\n","# Use it to process our training examples\n","pca.fit(train_examples)\n","\n","# Plot the cumulative explained variance against the number of PCs\n","plt.figure()\n","plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_)*100)\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Cumulative Explained Variance vs Number of Principal Components')\n","plt.grid(True)\n","plt.show()\n","\n","# Let's assume we decide to use the first 2 PCs based on the plot above...\n","\n","# Calculate the means of all the training feature values\n","means = np.mean(train_examples[:, :], axis=0)\n","\n","# Shift the training data and the testing data using the same transformation\n","train_examples_centred = train_examples - means\n","\n","# Shift the testing data and the testing data using the same transformation\n","test_examples_centred = test_examples - means\n","\n","# Note scikit-learn's .components_ attribute actually holds one PC per row... Hence us transposing it to look as we expect, below\n","\n","# Transform the original training examples to the new 2D space (matrix multiply between centred data and principle components)\n","train_examples = train_examples_centred @ pca.components_.T\n","\n","# And \"delete\" everything after the 2nd column\n","train_examples = train_examples[:, :2]\n","\n","# Or we could have multiplied with only the first two PCs... Let's use that approach to transform the testing examples\n","test_examples = test_examples_centred @ pca.components_[:2, :].T\n","\n","# model training and model evaluation steps\n","\n","# Create a Decision Tree model object:\n","model = DecisionTreeClassifier(random_state=99)\n","\n","# Call the model's fitting algorithm, passing in our training examples and training labels\n","model.fit(train_examples, train_labels)\n","\n","# Use the trained model to generate predictions for our testing examples\n","predictions = model.predict(test_examples)\n","\n","# Find the total number of model predictions that matched with the corresponding testing labels\n","correct_predictions = sum(predictions == test_labels)\n","# Calculate the model's accuracy: the fraction of predictions that were correct\n","accuracy = correct_predictions / len(test_labels)\n","# Display the accuracy as a single quantitative measure of overall performance\n","print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n","\n","# visualise the final generalisation\n","plt.figure()\n","disp = DecisionBoundaryDisplay.from_estimator(model, train_examples)\n","sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels)\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title('Decision tree: final generalisation')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"e9aeef52","metadata":{"id":"e9aeef52"},"outputs":[],"source":["# try some supervised learning with PCA and standardisation\n","\n","# data preparation steps\n","\n","# Importing the packages we use\n","import pandas as pd\n","import numpy as np\n","from sklearn.decomposition import PCA\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","from sklearn.inspection import DecisionBoundaryDisplay\n","import seaborn as sns\n","\n","# Do we standardise (using z-score standardisation), or not\n","Z = True\n","\n","# Loading all the observations\n","observations = pd.read_csv('iris.csv')\n","\n","# Shuffling all the observations\n","observations_shuffled = observations.sample(frac=1, random_state=99)\n","\n","# Setting the fraction of observations we will use for testing\n","testing_fraction = 0.25\n","split_index = int(observations_shuffled.shape[0] * testing_fraction)\n","\n","# Splitting into testing observations and training observations (\"horizontal split\")\n","observations_test = observations_shuffled.iloc[:split_index]\n","observations_train = observations_shuffled.iloc[split_index:]\n","\n","# Splitting into testing examples and testing labels (\"vertical split\")\n","test_examples = observations_test.drop(columns='species').to_numpy()\n","test_labels = observations_test['species'].to_numpy()\n","\n","# Splitting into training examples and training labels (\"vertical split\")\n","train_examples = observations_train.drop(columns='species').to_numpy()\n","train_labels = observations_train['species'].to_numpy()\n","\n","# Apply PCA to our training data\n","\n","if Z:\n","    scaler = StandardScaler()\n","    # z-score standardise the training examples (and store the parameters)\n","    train_examples = scaler.fit_transform(train_examples)\n","\n","# Create a PCA model object\n","pca = PCA()\n","\n","# Use it to process our training examples\n","pca.fit(train_examples)\n","\n","# Plot the cumulative explained variance against the number of PCs\n","plt.figure()\n","plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_)*100)\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Cumulative Explained Variance vs Number of Principal Components')\n","plt.grid(True)\n","plt.show()\n","\n","# Let's assume we decide to use the first 2 PCs based on the plot above...\n","\n","# Create a PCA model object that only retains the first N PCs\n","N = 2\n","pca = PCA(n_components=N)\n","\n","# Use it to process our training examples\n","pca.fit(train_examples)\n","\n","# Transform the original training examples to the new ND space\n","train_examples = pca.transform(train_examples)\n","\n","if Z:\n","    # z-score standardise the testing examples (using the stored parameters)\n","    test_examples = scaler.transform(test_examples)\n","\n","# Transform the original testing examples to the new ND space\n","test_examples = pca.transform(test_examples)\n","\n","# model training and model evaluation steps\n","\n","# Create a Decision Tree model object:\n","model = DecisionTreeClassifier(random_state=99)\n","\n","# Call the model's fitting algorithm, passing in our training examples and training labels\n","model.fit(train_examples, train_labels)\n","\n","# Use the trained model to generate predictions for our testing examples\n","predictions = model.predict(test_examples)\n","\n","# Find the total number of model predictions that matched with the corresponding testing labels\n","correct_predictions = sum(predictions == test_labels)\n","# Calculate the model's accuracy: the fraction of predictions that were correct\n","accuracy = correct_predictions / len(test_labels)\n","# Display the accuracy as a single quantitative measure of overall performance\n","print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n","\n","# visualise the final generalisation\n","plt.figure()\n","disp = DecisionBoundaryDisplay.from_estimator(model, train_examples)\n","sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels)\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.title('Decision tree: final generalisation')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}