{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e532b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change current working directory\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/AI/5-B-AI/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check we can see the dataset\n",
    "os.path.isfile('sepal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with a Logistic Regression classifier\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# Importing the packages we use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading all the observations\n",
    "observations = pd.read_csv('moons.csv')\n",
    "\n",
    "# Shuffling all the observations\n",
    "observations_shuffled = observations.sample(frac=1, random_state=99)\n",
    "\n",
    "# Setting the fraction of observations we will use for testing\n",
    "testing_fraction = 0.25\n",
    "split_index = int(observations_shuffled.shape[0] * testing_fraction)\n",
    "\n",
    "# Splitting into testing observations and training observations (\"horizontal split\")\n",
    "observations_test = observations_shuffled.iloc[:split_index]\n",
    "observations_train = observations_shuffled.iloc[split_index:]\n",
    "\n",
    "# Splitting into testing examples and testing labels (\"vertical split\")\n",
    "test_examples = observations_test.drop(columns='Label').to_numpy()\n",
    "test_labels = observations_test['Label'].to_numpy()\n",
    "\n",
    "# Splitting into training examples and training labels (\"vertical split\")\n",
    "train_examples = observations_train.drop(columns='Label').to_numpy()\n",
    "train_labels = observations_train['Label'].to_numpy()\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Create a Logistic Regression model object (the extra hyperparameter asks to use the \"cross entropy\" cost function...\n",
    "# (equivalent to the \"log loss\" where there are just two classes) we saw in the lecture, during gradient descent)\n",
    "model = LogisticRegression(multi_class='multinomial', random_state=99)\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(train_examples, train_labels)\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "# Find the total number of model predictions that matched with the corresponding testing labels\n",
    "correct_predictions = sum(predictions == test_labels)\n",
    "# Calculate the model's accuracy: the fraction of predictions that were correct\n",
    "accuracy = correct_predictions / len(test_labels)\n",
    "# Display the accuracy as a single quantitative measure of overall performance\n",
    "print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n",
    "\n",
    "# visualise the final generalisation (note the small change here...)\n",
    "plt.figure()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, train_examples, response_method='predict')\n",
    "sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistic regressor: final generalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with a Logistic Regression classifier (extra features)\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# Importing the packages we use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading all the observations\n",
    "observations = pd.read_csv('moons.csv')\n",
    "\n",
    "# Shuffling all the observations\n",
    "observations_shuffled = observations.sample(frac=1, random_state=99)\n",
    "\n",
    "# Setting the fraction of observations we will use for testing\n",
    "testing_fraction = 0.25\n",
    "split_index = int(observations_shuffled.shape[0] * testing_fraction)\n",
    "\n",
    "# Splitting into testing observations and training observations (\"horizontal split\")\n",
    "observations_test = observations_shuffled.iloc[:split_index]\n",
    "observations_train = observations_shuffled.iloc[split_index:]\n",
    "\n",
    "# Splitting into testing examples and testing labels (\"vertical split\")\n",
    "test_examples = observations_test.drop(columns='Label').to_numpy()\n",
    "test_labels = observations_test['Label'].to_numpy()\n",
    "\n",
    "# Splitting into training examples and training labels (\"vertical split\")\n",
    "train_examples = observations_train.drop(columns='Label').to_numpy()\n",
    "train_labels = observations_train['Label'].to_numpy()\n",
    "\n",
    "# generate extra predictive features using simple nonlinear transformations (multiplying existing predictive feature \n",
    "# values by themselves and each other), for both our training data and our testing data\n",
    "poly = PolynomialFeatures(2)\n",
    "poly_train_examples = poly.fit_transform(train_examples)\n",
    "poly_test_examples = poly.transform(test_examples)\n",
    "\n",
    "print(\"we've gone from\", train_examples.shape[1], \"predictive features, to\", poly_train_examples.shape[1])\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Create a Logistic Regression model object (the extra hyperparameter asks to use the \"cross entropy\" cost function...\n",
    "# (equivalent to the \"log loss\" where there are just two classes) we saw in the lecture, during gradient descent)\n",
    "model = LogisticRegression(multi_class='multinomial', random_state=99)\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(poly_train_examples, train_labels)\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(poly_test_examples)\n",
    "\n",
    "# Find the total number of model predictions that matched with the corresponding testing labels\n",
    "correct_predictions = sum(predictions == test_labels)\n",
    "# Calculate the model's accuracy: the fraction of predictions that were correct\n",
    "accuracy = correct_predictions / len(test_labels)\n",
    "# Display the accuracy as a single quantitative measure of overall performance\n",
    "print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n",
    "\n",
    "# visualise the final generalisation \n",
    "\n",
    "# (this is not something you'd ever need to do outside of a learning-about-ML context...\n",
    "# so don't worry about the code (which is very similar to what we did for regression visualisation), but some comments\n",
    "# follow if you're interested: \n",
    "# this is more involved now, a helper function like DecisionBoundaryDisplay\n",
    "# can't cope with >2 predictive featuers, so we'll have to do the visualisation ourselves and manually strip out all the \n",
    "# extra features before plotting; we do something very close to what we did for regression visualisation but this is a \n",
    "# classification problem, and we show the probability scores for the positive class (the one that comes first \n",
    "# alphabetically, by default - so \"Class A\"); (annoyingly contourf() won't accept string-like class labels, so we can't \n",
    "# (easily) show the boundary by plotting with two discrete colour codes across the grid, based on predicted labels \n",
    "# (at least not easily without a lot of messing round and recasting)))\n",
    "\n",
    "# Prepare our own grid of testing examples to use in visualization\n",
    "x_min = train_examples[:, 0].min() - 1\n",
    "x_max = train_examples[:, 0].max() + 1\n",
    "y_min = train_examples[:, 1].min() - 1\n",
    "y_max = train_examples[:, 1].max() + 1\n",
    "grid_x, grid_y = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "grid_examples = np.c_[grid_x.ravel(), grid_y.ravel()]\n",
    "\n",
    "# Apply the same nonlinear transformations to the feature values in our grid\n",
    "poly_grid_examples = poly.transform(grid_examples)\n",
    "\n",
    "# Find positive class probabilities for each of the examples in the grid\n",
    "grid_probabilities = model.predict_proba(poly_grid_examples)[:,0]\n",
    "grid_probabilities = grid_probabilities.reshape(grid_x.shape)\n",
    "\n",
    "# Visualise the final generalisation\n",
    "plt.figure()\n",
    "contour = plt.contourf(grid_x, grid_y, grid_probabilities, cmap='gray', alpha=0.8)\n",
    "plt.colorbar(contour, label='Positive class (A) probability')\n",
    "sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels, edgecolor='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistic regression (extra features): final generalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with a Support Vector Machine classifier\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# Importing the packages we use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading all the observations\n",
    "observations = pd.read_csv('moons.csv')\n",
    "\n",
    "# Shuffling all the observations\n",
    "observations_shuffled = observations.sample(frac=1, random_state=99)\n",
    "\n",
    "# Setting the fraction of observations we will use for testing\n",
    "testing_fraction = 0.25\n",
    "split_index = int(observations_shuffled.shape[0] * testing_fraction)\n",
    "\n",
    "# Splitting into testing observations and training observations (\"horizontal split\")\n",
    "observations_test = observations_shuffled.iloc[:split_index]\n",
    "observations_train = observations_shuffled.iloc[split_index:]\n",
    "\n",
    "# Splitting into testing examples and testing labels (\"vertical split\")\n",
    "test_examples = observations_test.drop(columns='Label').to_numpy()\n",
    "test_labels = observations_test['Label'].to_numpy()\n",
    "\n",
    "# Splitting into training examples and training labels (\"vertical split\")\n",
    "train_examples = observations_train.drop(columns='Label').to_numpy()\n",
    "train_labels = observations_train['Label'].to_numpy()\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Create a Support Vector Machine model object (generates extra predictive features by default)\n",
    "model = SVC(random_state=99)\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(train_examples, train_labels)\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "# Find the total number of model predictions that matched with the corresponding testing labels\n",
    "correct_predictions = sum(predictions == test_labels)\n",
    "# Calculate the model's accuracy: the fraction of predictions that were correct\n",
    "accuracy = correct_predictions / len(test_labels)\n",
    "# Display the accuracy as a single quantitative measure of overall performance\n",
    "print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n",
    "\n",
    "# visualise the final generalisation (note the small change here...)\n",
    "plt.figure()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, train_examples, response_method='predict')\n",
    "sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Support Vector Machine: final generalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with a Support Vector Machine regressor\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# Importing the packages we use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading all the observations\n",
    "observations = pd.read_csv('sepal_regression.csv')\n",
    "\n",
    "# Shuffling all the observations\n",
    "observations_shuffled = observations.sample(frac=1, random_state=99)\n",
    "\n",
    "# Setting the fraction of observations we will use for testing\n",
    "testing_fraction = 0.25\n",
    "split_index = int(observations_shuffled.shape[0] * testing_fraction)\n",
    "\n",
    "# Splitting into testing observations and training observations (\"horizontal split\")\n",
    "observations_test = observations_shuffled.iloc[:split_index]\n",
    "observations_train = observations_shuffled.iloc[split_index:]\n",
    "\n",
    "# Splitting into testing examples and testing labels (\"vertical split\")\n",
    "test_examples = observations_test.drop(columns='petal_length').to_numpy()\n",
    "test_values = observations_test['petal_length'].to_numpy()\n",
    "\n",
    "# Splitting into training examples and training labels (\"vertical split\")\n",
    "train_examples = observations_train.drop(columns='petal_length').to_numpy()\n",
    "train_values = observations_train['petal_length'].to_numpy()\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Create a Support Vector Machine model object (generates extra predictive features by default)\n",
    "model = SVR()\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(train_examples, train_values)\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "# Find the mean squared error (MSE) between the model's predictions and the testing values\n",
    "mse = ((predictions - test_values) ** 2).mean()\n",
    "# Display the MSE as a single quantitative measure of overall performance\n",
    "print(\"Mean square error (MSE):\", mse)\n",
    "\n",
    "# Prepare our own grid of testing examples to use in visualization\n",
    "x_min = train_examples[:, 0].min() - 1\n",
    "x_max = train_examples[:, 0].max() + 1\n",
    "y_min = train_examples[:, 1].min() - 1\n",
    "y_max = train_examples[:, 1].max() + 1\n",
    "grid_x, grid_y = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "grid_examples = np.c_[grid_x.ravel(), grid_y.ravel()]\n",
    "\n",
    "# Make predictions for each of the examples in the grid\n",
    "grid_predictions = model.predict(grid_examples)\n",
    "grid_predictions = grid_predictions.reshape(grid_x.shape)\n",
    "\n",
    "# Visualise the final generalisation\n",
    "plt.figure()\n",
    "contour = plt.contourf(grid_x, grid_y, grid_predictions, cmap='gray_r', alpha=0.8)\n",
    "plt.colorbar(contour, label='Petal Length')\n",
    "sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_values, palette='gray_r', edgecolor='k', legend=False)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Support Vector Machine: final generalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with an Artificial Neural Network classifier\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# Importing the packages we use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading all the observations\n",
    "observations = pd.read_csv('moons.csv')\n",
    "\n",
    "# Shuffling all the observations\n",
    "observations_shuffled = observations.sample(frac=1, random_state=99)\n",
    "\n",
    "# Setting the fraction of observations we will use for testing\n",
    "testing_fraction = 0.25\n",
    "split_index = int(observations_shuffled.shape[0] * testing_fraction)\n",
    "\n",
    "# Splitting into testing observations and training observations (\"horizontal split\")\n",
    "observations_test = observations_shuffled.iloc[:split_index]\n",
    "observations_train = observations_shuffled.iloc[split_index:]\n",
    "\n",
    "# Splitting into testing examples and testing labels (\"vertical split\")\n",
    "test_examples = observations_test.drop(columns='Label').to_numpy()\n",
    "test_labels = observations_test['Label'].to_numpy()\n",
    "\n",
    "# Splitting into training examples and training labels (\"vertical split\")\n",
    "train_examples = observations_train.drop(columns='Label').to_numpy()\n",
    "train_labels = observations_train['Label'].to_numpy()\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Create an Artificial Neural Network model object (using hyperparameters to ask for no hidden layers,\n",
    "# a logistic activation function, and stochastic gradient descent - equivalent to a logistic regressor)\n",
    "model = MLPClassifier(hidden_layer_sizes=(), activation='logistic', solver='sgd', random_state=99)\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(train_examples, train_labels)\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "# Find the total number of model predictions that matched with the corresponding testing labels\n",
    "correct_predictions = sum(predictions == test_labels)\n",
    "# Calculate the model's accuracy: the fraction of predictions that were correct\n",
    "accuracy = correct_predictions / len(test_labels)\n",
    "# Display the accuracy as a single quantitative measure of overall performance\n",
    "print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n",
    "\n",
    "# visualise the final generalisation (note the small change here...)\n",
    "plt.figure()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, train_examples, response_method='predict')\n",
    "sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Artificial Neural Network: final generalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdce1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with an Artificial Neural Network classifier\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# Importing the packages we use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading all the observations\n",
    "observations = pd.read_csv('moons.csv')\n",
    "\n",
    "# Shuffling all the observations\n",
    "observations_shuffled = observations.sample(frac=1, random_state=99)\n",
    "\n",
    "# Setting the fraction of observations we will use for testing\n",
    "testing_fraction = 0.25\n",
    "split_index = int(observations_shuffled.shape[0] * testing_fraction)\n",
    "\n",
    "# Splitting into testing observations and training observations (\"horizontal split\")\n",
    "observations_test = observations_shuffled.iloc[:split_index]\n",
    "observations_train = observations_shuffled.iloc[split_index:]\n",
    "\n",
    "# Splitting into testing examples and testing labels (\"vertical split\")\n",
    "test_examples = observations_test.drop(columns='Label').to_numpy()\n",
    "test_labels = observations_test['Label'].to_numpy()\n",
    "\n",
    "# Splitting into training examples and training labels (\"vertical split\")\n",
    "train_examples = observations_train.drop(columns='Label').to_numpy()\n",
    "train_labels = observations_train['Label'].to_numpy()\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Create an Artificial Neural Network model object (using hyperparameters to ask for one hidden layer\n",
    "# with 4 nodes, and a good number of epochs for the gradient descent)\n",
    "model = MLPClassifier(hidden_layer_sizes=(4), max_iter=2000, random_state=99)\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(train_examples, train_labels)\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "# Find the total number of model predictions that matched with the corresponding testing labels\n",
    "correct_predictions = sum(predictions == test_labels)\n",
    "# Calculate the model's accuracy: the fraction of predictions that were correct\n",
    "accuracy = correct_predictions / len(test_labels)\n",
    "# Display the accuracy as a single quantitative measure of overall performance\n",
    "print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n",
    "\n",
    "# visualise the final generalisation (note the small change here...)\n",
    "plt.figure()\n",
    "disp = DecisionBoundaryDisplay.from_estimator(model, train_examples, response_method='predict')\n",
    "sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_labels)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Artificial Neural Network: final generalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with an Artificial Neural Network regressor\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# Importing the packages we use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Loading all the observations\n",
    "observations = pd.read_csv('sepal_regression.csv')\n",
    "\n",
    "# Shuffling all the observations\n",
    "observations_shuffled = observations.sample(frac=1, random_state=99)\n",
    "\n",
    "# Setting the fraction of observations we will use for testing\n",
    "testing_fraction = 0.25\n",
    "split_index = int(observations_shuffled.shape[0] * testing_fraction)\n",
    "\n",
    "# Splitting into testing observations and training observations (\"horizontal split\")\n",
    "observations_test = observations_shuffled.iloc[:split_index]\n",
    "observations_train = observations_shuffled.iloc[split_index:]\n",
    "\n",
    "# Splitting into testing examples and testing labels (\"vertical split\")\n",
    "test_examples = observations_test.drop(columns='petal_length').to_numpy()\n",
    "test_values = observations_test['petal_length'].to_numpy()\n",
    "\n",
    "# Splitting into training examples and training labels (\"vertical split\")\n",
    "train_examples = observations_train.drop(columns='petal_length').to_numpy()\n",
    "train_values = observations_train['petal_length'].to_numpy()\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Create an Artificial Neural Network model object (you should be able to interpret the hyperparameters)\n",
    "model = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=2000, random_state=99)\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(train_examples, train_values)\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "# Find the mean squared error (MSE) between the model's predictions and the testing values\n",
    "mse = ((predictions - test_values) ** 2).mean()\n",
    "# Display the MSE as a single quantitative measure of overall performance\n",
    "print(\"Mean square error (MSE):\", mse)\n",
    "\n",
    "# Prepare our own grid of testing examples to use in visualization\n",
    "x_min = train_examples[:, 0].min() - 1\n",
    "x_max = train_examples[:, 0].max() + 1\n",
    "y_min = train_examples[:, 1].min() - 1\n",
    "y_max = train_examples[:, 1].max() + 1\n",
    "grid_x, grid_y = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "grid_examples = np.c_[grid_x.ravel(), grid_y.ravel()]\n",
    "\n",
    "# Make predictions for each of the examples in the grid\n",
    "grid_predictions = model.predict(grid_examples)\n",
    "grid_predictions = grid_predictions.reshape(grid_x.shape)\n",
    "\n",
    "# Visualise the final generalisation\n",
    "plt.figure()\n",
    "contour = plt.contourf(grid_x, grid_y, grid_predictions, cmap='gray_r', alpha=0.8)\n",
    "plt.colorbar(contour, label='Petal Length')\n",
    "sns.scatterplot(x=train_examples[:, 0], y=train_examples[:, 1], hue=train_values, palette='gray_r', edgecolor='k', legend=False)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title('Support Vector Machine: final generalisation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with a simple/small image dataset\n",
    "\n",
    "# https://en.wikipedia.org/wiki/MNIST_database - handwritten characters\n",
    "\n",
    "# data preparation steps\n",
    "\n",
    "# note: normally we would need to load individual image files, and read out and \"flatten\" the pixel information\n",
    "# into examples with lots of predictive features (e.g., the MNIST images are of resolution 28*28 pixels, leading \n",
    "# to examples with 784 predictive features)... Here, however, just to get the basic ideas quickly, we allow ourselves\n",
    "# to skip over this step and load up the images from a freely available online file resource, where they have already \n",
    "# been \"flattened\" and associated with their corresponding class labels (the number the person was writing - see the \n",
    "# link above for details). We also allow ourselves to use scikit-learn's train_test_split() method for doing the \n",
    "# shuffle and split - we've avoided doing this before to ensure we saw and understood the steps for ourselves... But \n",
    "# by this point we know the recipe basics and it's fine to make use of the helper...\n",
    "\n",
    "# Importing the packages we use\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Loading the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "\n",
    "# Examples and labels\n",
    "examples = mnist['data']\n",
    "labels = mnist['target']\n",
    "\n",
    "# Splitting into testing examples and training examples with corresponding labels\n",
    "train_examples, test_examples, train_labels, test_labels = train_test_split(examples, labels, test_size=0.25, random_state=99)\n",
    "\n",
    "# model training and model evaluation steps\n",
    "\n",
    "# Start timing\n",
    "train_start_time = time.time()\n",
    "\n",
    "# Create a k-NN model object\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Call the model's fitting algorithm, passing in our training examples and training labels\n",
    "model.fit(train_examples, train_labels)\n",
    "\n",
    "# Stop timing\n",
    "train_end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time for training\n",
    "train_time = train_end_time - train_start_time\n",
    "print(\"Training took\", train_time, \"seconds\")\n",
    "\n",
    "# Start timing for prediction\n",
    "predict_start_time = time.time()\n",
    "\n",
    "# Use the trained model to generate predictions for our testing examples\n",
    "predictions = model.predict(test_examples)\n",
    "\n",
    "# Stop timing for prediction\n",
    "predict_end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time for prediction\n",
    "predict_time = predict_end_time - predict_start_time\n",
    "print(\"Prediction took\", predict_time, \"seconds\")\n",
    "\n",
    "# Find the total number of model predictions that matched with the corresponding testing labels\n",
    "correct_predictions = sum(predictions == test_labels)\n",
    "# Calculate the model's accuracy: the fraction of predictions that were correct\n",
    "accuracy = correct_predictions / len(test_labels)\n",
    "# Display the accuracy as a single quantitative measure of overall performance\n",
    "print(\"Accuracy:\", accuracy, \"(or\", round(accuracy*100, 1), \"%)\")\n",
    "\n",
    "# WARNING: though this is a relatively small dataset, containing tiny images...\n",
    "# ...the code will still take a while to execute (and may take longer if you...\n",
    "# ...experiment with other models...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
